{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad8b343-4248-474c-a1f5-8fab31cf356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6d369-72d1-4cb4-b48e-90d06a8da770",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "__Word-level tokenization always performs better (i.e., gives us a better model) than character-level tokenization.__\n",
    "\n",
    "Answear: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b87985-15e9-4d59-b213-3fc2280017b1",
   "metadata": {},
   "source": [
    "#### Special Characters\n",
    "__When we process text data, we must always remove all numbers, all punctuation and all non-Unicode characters, in order to make our work easier.__\n",
    "\n",
    "Answear: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf71b8e-5a43-4aac-9c2b-4a052934f4bd",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "__Which of the following is / are true for the bag of words model (CountVectorizer in scikit-learn)?__\n",
    "- It represents each document as a list of tokens\n",
    "- It gives us word embeddings\n",
    "- It provides an entropy-like score for each document\n",
    "- It provides an entropy-like score for each word (token)\n",
    "- It preserves the order of the words in the document\n",
    "- It typically creates a \"low-sparsity\" matrix (i.e., few elements are zero)\n",
    "- It is only suitable for Romance (and other Indo-European) languages\n",
    "- All of the above\n",
    "- None of the above\n",
    "\n",
    "Answear: None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6f330-9a9e-4f43-889c-7d17e018a8f2",
   "metadata": {},
   "source": [
    "#### Language Models\n",
    "__Large language models (LLMs) are all the rage now. Very often, they use a simple method which allows them to work with very large sequences of text, \"focusing\" on specific parts of the inputs by weighting them; essentially - 1D convolution on a large scale. What is the name of this method?__\n",
    "- Word2Vec\n",
    "- Memory\n",
    "- Stochastic gradient descent\n",
    "- Attention\n",
    "- Direct preference optimization\n",
    "- Bi-directional embedding\n",
    "- Encoder-decoder\n",
    "- Transfer learning\n",
    "- Reinforcement learning from human feedback\n",
    "- None of the above\n",
    "\n",
    "Answear: Attention\n",
    "\n",
    "Attention mechanisms allow large language models to focus on specific parts of the input sequence by assigning different weights to different parts, enabling the model to handle long sequences of text more effectively. This method is particularly powerful in transformer architectures, which are the foundation of many state-of-the-art language models like GPT and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7263e89-6cb9-42bb-abac-e4d1621f3d57",
   "metadata": {},
   "source": [
    "#### Pride and Prejudice, Part 1\n",
    "__The most popular text in Project Gutenberg is the book \"Pride and Prejudice\" by Jane Austen, located at http://www.gutenberg.org/files/1342/1342-0.txt. Split by non-word characters to get all words. How many times does the word \"pride\" occur in the entire Web page? Don't forget to account for different casing.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a0c956-6966-4c7f-bf3c-3fc9d20d7334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Download the text from Project Gutenberg\n",
    "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Normalize the text to lowercase\n",
    "text_lower = text.lower()\n",
    "\n",
    "# Split the text into words using non-word characters as delimiters\n",
    "words = re.split(r'\\W+', text_lower)\n",
    "\n",
    "# Count the occurrences of the word \"pride\"\n",
    "pride_count = words.count(\"pride\")\n",
    "\n",
    "pride_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7717e-99fc-47f2-9311-c7a31cd15539",
   "metadata": {},
   "source": [
    "Answear: 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc8909-0124-40fe-a7a6-5cb6e72ed446",
   "metadata": {},
   "source": [
    "#### Pride and Prejudice, Part 2\n",
    "__How many times does the word \"prejudice\" occur in the entire Web page?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5526fa58-926a-44a2-b4ee-88d419b447a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'prejudice' occurs 10 times in the text.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Step 1: Download the text from Project Gutenberg\n",
    "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Step 2: Normalize the text to lowercase\n",
    "text_lower = text.lower()\n",
    "\n",
    "# Step 3: Split the text into words using non-word characters as delimiters\n",
    "words = re.split(r'\\W+', text_lower)\n",
    "\n",
    "# Step 4: Count the occurrences of the word \"prejudice\"\n",
    "prejudice_count = words.count(\"prejudice\")\n",
    "\n",
    "print(f\"The word 'prejudice' occurs {prejudice_count} times in the text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea7b8e-cbcf-4140-94e2-86e05ea4d826",
   "metadata": {},
   "source": [
    "#### Protagonist\n",
    "__Remove all stopwords. Once again, count all remaining words. What's the name of the protagonist (main character) in the book?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0080379a-67de-4d61-88f1-855e3a667519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\galin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('â', 3732), ('mr', 807), ('elizabeth', 605), ('could', 530), ('would', 482), ('said', 406), ('darcy', 383), ('mrs', 353), ('much', 335), ('miss', 315)]\n",
      "The protagonist's name is likely: â\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Download the text from Project Gutenberg\n",
    "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Step 2: Normalize the text to lowercase\n",
    "text_lower = text.lower()\n",
    "\n",
    "# Step 3: Split the text into words using non-word characters as delimiters\n",
    "words = re.split(r'\\W+', text_lower)\n",
    "\n",
    "# Step 4: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word and word not in stop_words]\n",
    "\n",
    "# Step 5: Count the remaining words\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Step 6: Identify the most frequent word\n",
    "most_common_words = word_counts.most_common(10)\n",
    "print(\"Most common words:\", most_common_words)\n",
    "\n",
    "# Assuming the protagonist's name is among the most common words\n",
    "protagonist = most_common_words[0][0] if most_common_words else None\n",
    "print(f\"The protagonist's name is likely: {protagonist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3316074c-89f2-46d0-8f45-61ed49428efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\galin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('mr', 807), ('elizabeth', 645), ('could', 530), ('would', 482), ('darcy', 430), ('said', 406), ('mrs', 353), ('bennet', 339), ('much', 335), ('miss', 315)]\n",
      "The protagonist's name is likely: mr\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Download the text from Project Gutenberg\n",
    "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.content.decode('utf-8')\n",
    "\n",
    "# Step 2: Normalize the text to lowercase and fix encoding issues\n",
    "text_lower = text.lower()\n",
    "\n",
    "# Step 3: Split the text into words using non-word characters as delimiters\n",
    "words = re.split(r'\\W+', text_lower)\n",
    "\n",
    "# Step 4: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word and word not in stop_words]\n",
    "\n",
    "# Step 5: Count the remaining words\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Step 6: Identify the most frequent word\n",
    "most_common_words = word_counts.most_common(10)\n",
    "print(\"Most common words:\", most_common_words)\n",
    "\n",
    "# Assuming the protagonist's name is among the most common words\n",
    "protagonist = most_common_words[0][0] if most_common_words else None\n",
    "print(f\"The protagonist's name is likely: {protagonist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c011ed-aaf9-4026-83c6-81f4b09a974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\galin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('elizabeth', 645), ('could', 530), ('would', 482), ('darcy', 430), ('said', 406), ('bennet', 339), ('much', 335), ('must', 312), ('bingley', 310), ('jane', 302)]\n",
      "The protagonist's name is likely: elizabeth\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Download the text from Project Gutenberg\n",
    "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.content.decode('utf-8')\n",
    "\n",
    "# Step 2: Normalize the text to lowercase and fix encoding issues\n",
    "text_lower = text.lower()\n",
    "\n",
    "# Step 3: Split the text into words using non-word characters as delimiters\n",
    "words = re.split(r'\\W+', text_lower)\n",
    "\n",
    "# Step 4: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word and word not in stop_words]\n",
    "\n",
    "# Step 5: Remove common titles\n",
    "common_titles = {'mr', 'mrs', 'miss'}\n",
    "filtered_words = [word for word in filtered_words if word not in common_titles]\n",
    "\n",
    "# Step 6: Count the remaining words\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Step 7: Identify the most frequent word\n",
    "most_common_words = word_counts.most_common(10)\n",
    "print(\"Most common words:\", most_common_words)\n",
    "\n",
    "# Assuming the protagonist's name is among the most common words\n",
    "protagonist = most_common_words[0][0] if most_common_words else None\n",
    "print(f\"The protagonist's name is likely: {protagonist}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4a665-87a2-4dad-9b75-441298f8f8da",
   "metadata": {},
   "source": [
    "__Answear:__ elizabeth  !!! Not sure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
