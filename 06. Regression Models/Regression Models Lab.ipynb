{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a21845-5edc-410a-ab41-5196319ddec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3786fdb-7af6-4ce2-b730-90c60a5d896f",
   "metadata": {},
   "source": [
    "# Regression Models Lab\n",
    "## Linear and logistic regression: theory and practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6ff4b-e18a-4580-8c7c-54f4c7ef2dc9",
   "metadata": {},
   "source": [
    "In this lab you'll revisit and expand on your knowledge of modelling in general, as well as the fundamentals of linear and logistic regression. As a reminder, _linear regression_ is a regression model (regressor), and _logistic regression_ is a classification model (classifier).\n",
    "\n",
    "This time, you'll use generated data, in order to separate some of the complexity of handling various datasets from inspecting and evaluating models.\n",
    "\n",
    "**Use vectorization as much as possible!** You should be able to complete the lab using for-loops only to track the training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a9603-c803-4728-a69d-b6acfe2bad8a",
   "metadata": {},
   "source": [
    "### Problem 1. Generate some data for multiple linear regression (1 point)\n",
    "As an expansion to the lecture, you'll create a dataset and a model.\n",
    "\n",
    "Create a dataset of some (e.g., 50-500) observations of several (e.g., 5-20) independent features. You can use random generators for them; think about what distributions you'd like to use. Let's call them $x_1, x_2, ..., x_m$. The data matrix $X$ you should get should be of size $n \\times m$. It's best if all features have different ranges.\n",
    "\n",
    "Create the dependent variable by assigning coefficients $\\bar{a_1}, \\bar{a_2}, ..., \\bar{a_m}, \\bar{b}$ and calculating $y$ as a linear combination of the input features. Add some random noise to the functional values. I've used bars over coefficients to avoid confusion with the model parameters later.\n",
    "\n",
    "Save the dataset ($X$ and $y$), and \"forget\" that the coefficients have ever existed. \"All\" you have is the file and the implicit assumption that there is a linear relationship between $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d3ed2-0be7-4b9a-b893-6b955e6e8191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4bb4164-eaa3-4b76-ae3f-36da95416b7b",
   "metadata": {},
   "source": [
    "### Problem 2. Check your assumption (1 point)\n",
    "Read the dataset you just saved (this is just to simulate starting a new project). It's a good idea to test and verify our assumptions. Find a way to check whether there really is a linear relationship between the features and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c8452-5eb1-4719-88da-372331cfce82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b5f9a49-cee6-4537-b82f-d8148a45c0c2",
   "metadata": {},
   "source": [
    "### Problem 3. Figure out the modelling function (1 point)\n",
    "The modelling function for linear regression is of the form\n",
    "$$ \\tilde{y} = \\sum_{i=1}^{m}a_i x_i + b $$\n",
    "\n",
    "If you want to be clever, you can find a way to represent $b$ in the same way as the other coefficients.\n",
    "\n",
    "Write a Python function which accepts coefficients and data, and ensure (test) it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800a26d-3204-49ad-8a83-37edf0f4128f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ea9d650-0994-4ae8-bba9-3b8fdecbdd3e",
   "metadata": {},
   "source": [
    "### Problem 4. Write the cost function and compute its gradients (1 point)\n",
    "Use MSE as the cost function $J$. Find a way to compute, calculate, or derive its gradients w.r.t. the model parameters $a_1, ..., a_m, b$\n",
    "\n",
    "Note that computing the cost function value and its gradients are two separate operations. Quick reminder: use vectorization to compute all gradients (maybe with the exception of $\\frac{\\partial J}{\\partial b}$) at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99388ae1-1b23-4233-acf9-daf589bba07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bad786e-4299-4900-ae86-16cbd2235a6e",
   "metadata": {},
   "source": [
    "### Problem 5. Perform gradient descent (1 point)\n",
    "Perform weight updates iteratively. Find a useful criterion for stopping. For most cases, just using a fixed (large) number of steps is enough.\n",
    "\n",
    "You'll need to set a starting point (think about which one should be good, and how it matters); and a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bc3bd-6b2c-4f40-9e94-40557a2325d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d18a168f-de4c-4035-88db-7350c2b91745",
   "metadata": {},
   "source": [
    "### Problem 6. Do other cost functions work? (2 points)\n",
    "Repeat the process in problems 4 and 5 with MAE, and then again - with the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss). Both of them are less sensitive to outliers / anomalies than MSE); with the Huber loss function being specifically made for datasets with outliers.\n",
    "\n",
    "Explain your findings. Is there a cost function that works much better? How about speed of training (measured in wall time)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0eb74-7b3d-4aa8-9749-dcb55e918a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4176d1-6cad-4830-824c-1d27e50efe89",
   "metadata": {},
   "source": [
    "### Problem 7. Experiment with the learning rate (1 point)\n",
    "Use your favorite cost function. Run several \"experiments\" with different learning rates. Try really small, and really large values. Observe and document your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f04525-95f3-427d-8763-27af6839cb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91385302-2fc1-48a4-a453-cce7f038e9c2",
   "metadata": {},
   "source": [
    "### Problem 8. Generate some data for classification (1 point)\n",
    "You'll need to create two clusters of points (one cluster for each class). I recomment using `scikit-learn`'s `make_blobs()` ([info](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)). Use as many features as you used in problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948009d-6af6-468c-ac6e-bacd4f75498c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6246890-0808-40e1-b892-fa1d11fd088c",
   "metadata": {},
   "source": [
    "### Problem 9. Perform logistic regression (1 point)\n",
    "Reuse the code you wrote in problems 3-7 as much as possible. If you wrote vectorized functions with variable parameters - you should find this easy. If not - it's not too late to go back and refactor your code.\n",
    "\n",
    "The modelling function for logistic regression is\n",
    "$$ \\tilde{y} = \\frac{1}{1+\\exp{(-\\sum_{i=1}^{m}a_i x_i + b)}}$$. Find a way to represent it using as much of your previous code as you can.\n",
    "\n",
    "The most commonly used loss function is the [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy).\n",
    "\n",
    "Experiment with different learning rates, basically repeating what you did in problem 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e100b-1627-4300-8d2e-98e8b438e046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411df95f-5f16-4c37-8649-2c495eb96974",
   "metadata": {},
   "source": [
    "### * Problem 10. Continue experimenting and delving deep into ML\n",
    "You just saw how modelling works and how to implement some code. Some of the things you can think about (and I recommend you pause and ponder on some of them are):\n",
    "* Code: OOP can be your friend sometimes. `scikit-learn`'s models have `fit()`, `predict()` and `score()` methods.\n",
    "* Data: What approaches work on non-generated data?\n",
    "* Evaluation: How well do different models (and their \"settings\" - hyperparameters) actually work in practice? How do we evaluate a model in a meaningful way?\n",
    "* Optimization - maths: Look at what `optimizers` (or solvers) are used in `scikit-learn` and why. Many \"tricks\" revolve around making the algorithm converge (finish) in fewer iterations, or making it more numerically stable.\n",
    "* Optimization - code: Are there ways to make the code run fastr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08101fb6-d53f-4dc9-8f37-d07478c9e220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
