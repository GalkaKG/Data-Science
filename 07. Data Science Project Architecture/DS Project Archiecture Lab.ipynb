{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3852edca-85e7-4f61-8ccf-4163e8be9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985ac40-d91f-4214-8a75-d9ad4660887b",
   "metadata": {},
   "source": [
    "# Data Science Project Architecture\n",
    "## Getting a feel of an end-to-end data science solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f67874-d20d-45ef-abe4-df2332080e11",
   "metadata": {},
   "source": [
    "In this lab, you'll see how all the pieces of data science: data analysis, code, tooling, experiments, come together to create a complete project. You'll perform a smaller-scale demonstration of a data science project lifecycle. Of course, you have to keep in mind that \"real-life\" data science is highly iterative. You might be working on the same task(s) for weeks or months - this lab is not able to show that.\n",
    "\n",
    "You'll be working with the asthma dataset located [here](https://www.kaggle.com/datasets/rabieelkharoua/asthma-disease-dataset). As always, it's preloaded for you in the `data/` directory. **Your main goal is to predict what factors lead to positive diagnosis.**\n",
    "\n",
    "This time, I suggest you do your research into separate notebooks, not inside this one. Use one or several, as you see fit; there are no guidelines as to how many notebooks you should have, or how long (or short) they have to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c3885-cb90-456c-a34e-8c49ed2d8e50",
   "metadata": {},
   "source": [
    "### Problem 1. Project structure (1 point)\n",
    "Create the necessary directories and structure that you'll use to put your work in. I am providing a suggestion, but you don't have to follow it.\n",
    "`data/` for... data :D\n",
    "`notebooks/` for your research. Feel free to move this one inside.\n",
    "`src/` for Python code (which you'll need to create towards the end of the lab)\n",
    "`test/` (or `tests/`) for unit tests\n",
    "\n",
    "You may add any other structure you like. For inspiration, you can see how popular libraries handle their file structure.\n",
    "\n",
    "Create a GitHub repo (or any other Git-based source control, but I **highly** suggest GitHub) containing your initial project structure. Don't forget to commit relatively often so you have a way to track what you've done so far and go back if something goes awry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7505475-3e54-4aec-981e-8ce46b10eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to create\n",
    "files = [\n",
    "    'README.md',\n",
    "    'requirements.txt',\n",
    "    '.gitignore',\n",
    "    'src/__init__.py',\n",
    "    'src/data_preprocessing.py',\n",
    "    'src/model.py',\n",
    "    'src/evaluation.py',\n",
    "    'src/utils.py',\n",
    "    'test/__init__.py',\n",
    "    'test/test_data_preprocessing.py',\n",
    "    'test/test_model.py',\n",
    "    'test/test_evaluation.py'\n",
    "]\n",
    "\n",
    "# Create each file\n",
    "for file in files:\n",
    "    with open(file, 'w') as f:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826a79c-e60b-4e88-9bcc-6e203e0f9ac1",
   "metadata": {},
   "source": [
    "### Problem 2. Data Exploration (1 point)\n",
    "In an appropriate notebook, load the data. Ensure its validity and start your EDA. Feel free to create any visualizations, tables, filters, etc. you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e05662-789c-4ea9-994b-32cb0da3c25e",
   "metadata": {},
   "source": [
    "__The file with the solition is \"notebooks/\\_02_Data_Exploration.ipynb\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012b94f-539f-4013-837e-ed50d2b5ba0d",
   "metadata": {},
   "source": [
    "### Problem 3. Data cleaning and preprocessing (1 point)\n",
    "This should be self-explanatory. In an appropriate notebook (probably different than your previous one), explore different ways to clean and preprocess the dataset.\n",
    "\n",
    "This is still part of your research. That is, don't be afraid to _try out different approaches to the same problem_. E.g., if you have a lot of missing values, you may not know right away how to handle them. Experimenting with several approaches will give you a better indication what works well for your data and goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d8d1b-9684-44e6-b2d3-0126588ff436",
   "metadata": {},
   "source": [
    "__The file with the solition is \"notebooks/\\_03_Data_cleaning_and_preprocessing.ipynb\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825c9c4-570b-488c-b13d-41e494713853",
   "metadata": {},
   "source": [
    "### Problem 4. Exploratory data analysis (1 point)\n",
    "This step may, or may not, happen in unison with the previous one. Your goal is to understand the data distributions, relationships, useful features, maybe create visualizations and inform your data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb35fb4-fa01-46a9-aba1-0fb72ccf5f49",
   "metadata": {},
   "source": [
    "__The file with the solition is \"notebooks/\\_04_Exploratory_data_analysis.ipynb__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feacc8f7-a0f5-46b8-9967-9136eeb9fc8f",
   "metadata": {},
   "source": [
    "### Problem 5. Feature manipulation (1 point)\n",
    "Now that your data has been thoroughly cleaned (w.r.t. your goal to model diagnoses) and explored, you'll need to \"play around\" and prepare good features.\n",
    "\n",
    "You don't have to think about modelling (machine learning) at this stage (although it won't do harm). Perform feature selection and feature engineering in ways that you think will be beneficial for a \"mental\" model of the data. Such a model consists of hypotheses that you should be able to test.\n",
    "\n",
    "Feel free to do any sort of feature maniplulation on the data you like. Ideally, at the end of the process, you'll have a rectangular data table consisting of only (floating-point) numbers and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91a3f2-d760-4a95-811f-1f0011491e62",
   "metadata": {},
   "source": [
    "__The file with the solition is \"notebooks/\\_05_Feature_manipulation.ipynb__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4df3d0-b222-48f6-8af7-013ef8e84344",
   "metadata": {},
   "source": [
    "### Problem 6. Data preparation and manipulation script (2 points)\n",
    "So far, you should have tried lots of different ways to work with the data. Some of them should have been good, others - not so much. This is extremely valuable research, and we don't want to lose it, but now we have to think about automation.\n",
    "\n",
    "Extract your data preprocessing and manipulation functions into one or more files in the `src/` (or similar) directory. Debug the code and ensure it's been optimized. Use vectorization and the `pandas` / `numpy` APIs as much as possible. I don't usually expect data processing scripts to create visualizations. Most often, they consist of functions which accept some dataframe(s) and return (an)other dataframe(s). Also, we usually avoid one-liners (e.g., a function which only calls a different function) unless there's a very good reason for them (e.g., it's semantically easier to understand).\n",
    "\n",
    "Refactor the code so that it's **reusable**. Function parameters (and polymorphism) achieve a lot in terms of reusability :). Avoid hardcoding stuff. Follow the best practices in Python and the style guides. Use a linter to help you clean up your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bd3da-9534-4067-b65c-300f23f52c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a20ac09-21bc-4730-8986-3e186863c4ba",
   "metadata": {},
   "source": [
    "### Problem 7. Documentation (1 point)\n",
    "Ensure all your public-facing functions (that is, functions that are \"exposed\" to the user) have docstrings. Ensure they are well-documented and their purpose is clear. This is especially valuable if you're doing some advanced analysis or data manipulation. You can see various ways of creating docstrings online. There are even tools (e.g., VSCode extensions) which will help you with the docstring boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cd4a5-ee2a-485f-bdab-07e5e1d034d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "816f155c-98dd-4b59-ac8e-c429e760e75d",
   "metadata": {},
   "source": [
    "### Problem 8. Testing (1 point)\n",
    "Now that you've done the previous two problems, you have _specification_ (your documentation - it tells you what you intend to do) and _implementation_ (your well-written and refactored code - it tells you _how_ it's done). The difficult part now is to ensure these two things match.\n",
    "\n",
    "Create unit tests for your functions. Be careful so you test _your_ code, not `pandas`'. Create hypothesis tests to validate your assumptions. Do validity checks on the input data and sanity checks on the outputs of functions. Ensure your code is well-tested. Ensure it's modular, reusable, and flexible; but most of all - that it works **correctly**. If you haven't yet (though you should have - in problem 6) - think about exceptions and exception handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416eef1-00d7-4574-ace4-ffe3428b043f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc2a89bc-7a03-450e-a1a3-ca163f4f8fe6",
   "metadata": {},
   "source": [
    "### Problem 9. Reproducibility (1 point)\n",
    "Ensure all your notebooks and scripts are not only correct, but also reproducible. List all code dependencies (probably in a `requirements.txt` file); ensure your random seeds are correct; ensure the code produces the same results when run multiple times, etc.\n",
    "\n",
    "Do your final cleanup work. You might want to differentiate your \"draft\" noteoboks from your \"official\" ones (although I advise against that) and creat your final commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad899c1-a7bc-480a-a71b-49337ded85bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0491a01-a205-4713-87ee-d1e6d2dbf072",
   "metadata": {},
   "source": [
    "### * Problem 10. Above and beyond\n",
    "Of course, there are many things to be done. If you have time, I advise you learn how to work with data versioning (using DVC) and data pipeline / artifact tracking (using MLFlow or a similar tool). You might also find it useful to create a \"proper\", advanced data pipeline where you may need to work with big files (using Dask or a similar library), or schedule and organize tasks (using data pipeline managers like Luigi or Airflow).\n",
    "\n",
    "You might also want to do machine learning. I've deliberately stayed away from that for the purposes of the lab because it's a whole different beast, but it's a worthy challenge and it's extremely interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6bb04-06e0-4075-abcc-5bba12e97476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
